# A

- word: A/B testing
  trans: AB テスト
  kana: えーびー テスト
  def: |
    ２つ、もしくはそれ以上の手法を比較する統計的手法。既存の手法に対して新しい手法を比較することが多い。どちらの手法が良いかだけではなく、
    差が統計的に有意かどうかを理解するのが目的である。通常は、１つの指標を用いて２つの手法を比べるが、複数の手法や指標を対象とすることもできる。
  ref:
    - A/B testing (Machine Learning Glossary): https://developers.google.com/machine-learning/glossary/

- word: accuracy
  trans: 精度
  kana: せいど
  def: |
    分類において、正しく分類された予測の割合。複数クラス分類においては、精度は「正しい予測の数 ÷ 事例の総数」で求められる。
    ２クラス分類においては、「（真陽性 + 真偽性）÷ 事例の総数」とも定義される。
  ref:
    - accuracy (Machine Learning Glossary): https://developers.google.com/machine-learning/glossary/
    - true positive
    - true negative

- word: activation
  trans: 活性化
  kana: かっせいか
  def: ニューラル・ネットワークのノード (節点) の出力値。しばし活性化関数 (activation function) と混同して使用される。
  ref:
    - activation function

- word: activation function
  trans: 活性化関数
  kana: かっせいかかんすう
  def: ニューラル・ネットワークにおいて、線形層の後に適用される非線形関数のこと。ReLU、シグモイド関数などがある。
  ref:
    - activation
    - 活性化関数 (Wikipedia): https://ja.wikipedia.org/wiki/%E6%B4%BB%E6%80%A7%E5%8C%96%E9%96%A2%E6%95%B0
    - ReLU
    - sigmoid


# C

- word: class
  trans: クラス
  kana: くらす
  def: 分類問題において、データが属するグループ

- word: classification
  trans: 分類
  kana: ぶんるい
  def: データを複数のクラス（グループ）に仕分けること
  ref:
    - class

- word: convex optimization
  trans: 凸最適化
  kana: とつさいてきか
  def: 凸関数の最適化
  ref:
    - optimization

- word: dropout
  trans: ドロップアウト
  kana: どろっぷあうと
  def: ニューラル・ネットワークの訓練中に、ノードをランダムに欠損させる正則化テクニック。過学習を防ぐ効果がある。
  ref:
    - ドロップアウトに関するオリジナルの論文 (Srivastava et al. 2014): https://www.cs.toronto.edu/~hinton/absps/JMLRdropout.pdf
    - regularization


# E

- word: embedding
  trans: 埋め込み
  kana: うめこみ
  def: 単語やカテゴリ変数などの離散的なものを、実数のベクトルに変換すること。もしくは、その結果の実数ベクトル。


# G

- word: gradient
  trans: 勾配
  kana: こうばい
  def: 関数の変化率を表すベクトル
  ref:
    - 勾配 (Wikipedia): https://ja.wikipedia.org/wiki/%E5%8B%BE%E9%85%8D_(%E3%83%99%E3%82%AF%E3%83%88%E3%83%AB%E8%A7%A3%E6%9E%90)
    - stochastic gradient descent

- word: gradient descent
  trans: 勾配降下法; 最急降下法
  kana: こうばいこうかほう; 最急降下法
  def: 勾配を用いて関数の最小値を探索する最適化アルゴリズム。
  ref:
    - gradient
    - 最急降下法 (Wikipedia): https://ja.wikipedia.org/wiki/%E6%9C%80%E6%80%A5%E9%99%8D%E4%B8%8B%E6%B3%95


# E

- word: epoch
  trans: エポック
  kana: えぽっく
  def: 学習アルゴリズムに対して、全ての訓練データを一通り与えるサイクル。一つの訓練データを、複数回繰り返して提示するのが普通である。


# H

- word: hyperparameter
  trans: ハイパーパラメータ
  kana: はいぱーぱらめーた
  def: 機械学習アルゴリズムに関するパラメータ。学習されるモデルに関する通常のパラメータと対比してこう呼ばれる。


# L

- word: learning rate
  trans: 学習率
  kana: がくしゅうりつ
  def: 勾配法において、どのぐらいパラメータを変化させるかを制御するハイパーパラメータ。勾配に対する係数。

- word: least squares method
  trans: 最小二乗法
  kana: さいしょうにじょうほう
  def: 残差の二乗和を最小化するように最適化する方法
  ref:
    - 最小二乗法 (Wikipedia): https://ja.wikipedia.org/wiki/%E6%9C%80%E5%B0%8F%E4%BA%8C%E4%B9%97%E6%B3%95

- word: loss
  trans: 損失
  kana: そんしつ
  def: モデルの予測が、真のラベルからどのぐらい遠いかを表す量。コストとも呼ばれる。
  ref:
    - loss function
  syn:
    - cost

- word: loss function
  trans: 損失関数
  kana: そんしつかんすう
  def: モデルの予測と真のラベルが与えられた時、その両者がどのぐらい遠いかを表す量（損失）を返す関数。コスト関数とも呼ばれる。
  ref:
    - loss
  syn:
    - cost function


# M

- word: machine learning
  trans: 機械学習
  kana: きかいがくしゅう
  def: 人間が自然に行っている学習能力と同様の機能をコンピュータで実現しようとする技術・手法
  ref:
    - 機械学習 (Wikipedia): https://ja.wikipedia.org/wiki/%E6%A9%9F%E6%A2%B0%E5%AD%A6%E7%BF%92
  syn:
    - ML


# N

- word: neural networks
  trans: ニューラル・ネットワーク
  kana: にゅーらる・ねっとわーく
  def: 神経回路網から着想を得た、ノード（節点）とエッジ（結合）からなる計算のためのモデル。ノード間の接続の強さを調整することにより学習する。
  ref:
    - ニューラルネットワーク (Wikipedia): https://ja.wikipedia.org/wiki/%E3%83%8B%E3%83%A5%E3%83%BC%E3%83%A9%E3%83%AB%E3%83%8D%E3%83%83%E3%83%88%E3%83%AF%E3%83%BC%E3%82%AF


# O

- word: optimization
  trans: 最適化
  kana: さいてきか
  def: 関数・プログラム・製造物などを最適な状態に近づけること。特に、関数の値が最小もしくは最大となる状態を求めること。
  ref:
    - 最適化問題 (Wikipedia): https://ja.wikipedia.org/wiki/%E6%9C%80%E9%81%A9%E5%8C%96%E5%95%8F%E9%A1%8C

- word: overfitting
  trans: 過学習; 過剰適合
  kana: かがくしゅう; かじょうてきごう
  def: モデルが訓練データには適合しているが、未知のデータに対しては汎化能力の低い状態
  ref:
    - 過剰適合 (Wikipedia): https://ja.wikipedia.org/wiki/%E9%81%8E%E5%89%B0%E9%81%A9%E5%90%88
    - underfitting


# R

- word: regularization
  trans: 正則化
  kana: せいそくか
  def: 機械学習において、過学習を防ぐために、モデルに何らかの追加の制約を導入する方法。
  ref:
    - 正則化 (Wikipedia): https://ja.wikipedia.org/wiki/%E6%AD%A3%E5%89%87%E5%8C%96
    - dropout

- word: residual network
  trans: 残差ネットワーク
  kana: ざんさねっとわーく
  def: |
    畳み込み層をスキップし、入力信号をそのまま出力に足し合わせる「残差ブロック (residual block)」
    を用いて構築されたニューラル・ネットワーク。非常に深い（層の数が多い）ニューラル・ネットワークを訓練できる。
  ref:
    - 残差ネットワークに関するオリジナルの論文 (He at al. 2015): https://arxiv.org/abs/1512.03385


# S

- word: stochastic gradient descent
  trans: 確率的勾配降下法
  kana: かくりつてきこうばいこうかほう
  def: データの各インスタンスごとに勾配を計算し、関数の最小値を探索する勾配降下法の一つ。通常の勾配降下法のオンライン学習版とも言える。
  ref:
    - 確率的勾配降下法 (Wikipedia): https://ja.wikipedia.org/wiki/%E7%A2%BA%E7%8E%87%E7%9A%84%E5%8B%BE%E9%85%8D%E9%99%8D%E4%B8%8B%E6%B3%95
    - gradient descent


# U

- word: underfitting
  trans: 未学習
  kana: みがくしゅう
  def: モデルがデータの複雑さを捉えきれていなく、バイアスが高い状態。過学習の逆。
  ref:
    - overfitting
